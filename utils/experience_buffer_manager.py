"""
ê²½í—˜ ë²„í¼ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
====================

ê°•í™”í•™ìŠµ ê²½í—˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥/ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ ì œê³µ

ì£¼ìš” ê¸°ëŠ¥:
- HDF5, Pickle, NumPy í˜•ì‹ìœ¼ë¡œ ê²½í—˜ ë°ì´í„° ì €ì¥
- ë°ì´í„° ì••ì¶• ë° ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- ì˜¤ë˜ëœ íŒŒì¼ ìë™ ì •ë¦¬
- ë°°ì¹˜ ë¡œë”© ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
"""

import os
import time
import pickle
import numpy as np
import json
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime
import logging
import glob


class ExperienceBufferManager:
    """ê²½í—˜ ë²„í¼ ë°ì´í„° ì €ì¥/ë¡œë“œ ê´€ë¦¬ì"""
    
    def __init__(self, 
                 save_path: str = "data/experience_buffer",
                 format_type: str = "hdf5",
                 max_files: int = 50,
                 compress: bool = True):
        """
        Args:
            save_path: ì €ì¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            format_type: ì €ì¥ í˜•ì‹ ("hdf5", "pickle", "numpy")
            max_files: ìµœëŒ€ ë³´ê´€ íŒŒì¼ ê°œìˆ˜
            compress: ì••ì¶• ì—¬ë¶€
        """
        self.save_path = save_path
        self.format_type = format_type.lower()
        self.max_files = max_files
        self.compress = compress
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(save_path, exist_ok=True)
        
        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(__name__)
        
        # ì§€ì›í•˜ëŠ” í˜•ì‹ ê²€ì¦
        if self.format_type not in ["hdf5", "pickle", "numpy"]:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format_type}. ì§€ì› í˜•ì‹: hdf5, pickle, numpy")
        
        # HDF5 í˜•ì‹ ì‚¬ìš© ì‹œ h5py ì„í¬íŠ¸ ì‹œë„
        if self.format_type == "hdf5":
            try:
                import h5py
                self.h5py = h5py
            except ImportError:
                self.logger.warning("h5pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ pickle í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                self.format_type = "pickle"
    
    def save_experience_buffer(self, 
                              experiences: List[Dict[str, Any]], 
                              step_number: int,
                              metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        ê²½í—˜ ë²„í¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            experiences: ê²½í—˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            step_number: í˜„ì¬ í›ˆë ¨ ìŠ¤í… ë²ˆí˜¸
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if not experiences:
            self.logger.warning("ì €ì¥í•  ê²½í—˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + ìŠ¤í… ë²ˆí˜¸)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"experience_buffer_step{step_number:06d}_{timestamp}"
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        if metadata is None:
            metadata = {}
        
        metadata.update({
            "step_number": step_number,
            "timestamp": timestamp,
            "num_experiences": len(experiences),
            "format_type": self.format_type,
            "compressed": self.compress,
            "saved_at": time.time()
        })
        
        try:
            if self.format_type == "hdf5":
                filepath = self._save_hdf5(experiences, filename, metadata)
            elif self.format_type == "pickle":
                filepath = self._save_pickle(experiences, filename, metadata)
            elif self.format_type == "numpy":
                filepath = self._save_numpy(experiences, filename, metadata)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {self.format_type}")
            
            self.logger.info(f"âœ… ê²½í—˜ ë²„í¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            self.logger.info(f"   - ê²½í—˜ ê°œìˆ˜: {len(experiences):,}")
            self.logger.info(f"   - íŒŒì¼ í¬ê¸°: {self._get_file_size(filepath)}")
            
            # ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬
            self._cleanup_old_files()
            
            return filepath
            
        except Exception as e:
            self.logger.error(f"âŒ ê²½í—˜ ë²„í¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def load_experience_buffer(self, 
                              filepath: str = None,
                              step_number: int = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        ê²½í—˜ ë²„í¼ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ
        
        Args:
            filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê°€ì¥ ìµœê·¼ íŒŒì¼)
            step_number: íŠ¹ì • ìŠ¤í…ì˜ íŒŒì¼ ë¡œë“œ
            
        Returns:
            (ê²½í—˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°)
        """
        try:
            # íŒŒì¼ ê²½ë¡œ ê²°ì •
            if filepath is None:
                if step_number is not None:
                    filepath = self._find_file_by_step(step_number)
                else:
                    filepath = self._get_latest_file()
            
            if not filepath or not os.path.exists(filepath):
                self.logger.warning(f"ê²½í—˜ ë²„í¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
                return [], {}
            
            # íŒŒì¼ í˜•ì‹ ê°ì§€
            format_type = self._detect_file_format(filepath)
            
            if format_type == "hdf5":
                return self._load_hdf5(filepath)
            elif format_type == "pickle":
                return self._load_pickle(filepath)
            elif format_type == "numpy":
                return self._load_numpy(filepath)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {format_type}")
                
        except Exception as e:
            self.logger.error(f"âŒ ê²½í—˜ ë²„í¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return [], {}
    
    def list_experience_files(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ ê²½í—˜ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        files_info = []
        
        patterns = [
            os.path.join(self.save_path, "experience_buffer_*.h5"),
            os.path.join(self.save_path, "experience_buffer_*.pkl"),
            os.path.join(self.save_path, "experience_buffer_*.npz")
        ]
        
        for pattern in patterns:
            for filepath in glob.glob(pattern):
                try:
                    file_stat = os.stat(filepath)
                    file_info = {
                        "filepath": filepath,
                        "filename": os.path.basename(filepath),
                        "size": file_stat.st_size,
                        "size_mb": file_stat.st_size / (1024 * 1024),
                        "created_time": file_stat.st_ctime,
                        "modified_time": file_stat.st_mtime,
                        "format": self._detect_file_format(filepath)
                    }
                    
                    # ìŠ¤í… ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
                    try:
                        step_str = filepath.split("step")[1].split("_")[0]
                        file_info["step_number"] = int(step_str)
                    except:
                        file_info["step_number"] = -1
                    
                    files_info.append(file_info)
                    
                except Exception as e:
                    self.logger.warning(f"íŒŒì¼ ì •ë³´ ì½ê¸° ì‹¤íŒ¨ {filepath}: {e}")
        
        # ìŠ¤í… ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        files_info.sort(key=lambda x: x["step_number"])
        return files_info
    
    def _save_hdf5(self, experiences: List[Dict], filename: str, metadata: Dict) -> str:
        """HDF5 í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        filepath = os.path.join(self.save_path, f"{filename}.h5")
        
        with self.h5py.File(filepath, 'w') as f:
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta_group = f.create_group("metadata")
            for key, value in metadata.items():
                if isinstance(value, (str, int, float)):
                    meta_group.attrs[key] = value
                else:
                    meta_group.attrs[key] = str(value)
            
            # ê²½í—˜ ë°ì´í„° ì €ì¥
            exp_group = f.create_group("experiences")
            
            for i, experience in enumerate(experiences):
                exp_subgroup = exp_group.create_group(f"experience_{i}")
                
                for key, value in experience.items():
                    if isinstance(value, np.ndarray):
                        dataset = exp_subgroup.create_dataset(
                            key, data=value,
                            compression='gzip' if self.compress else None
                        )
                    elif isinstance(value, (int, float)):
                        exp_subgroup.attrs[key] = value
                    else:
                        exp_subgroup.attrs[key] = str(value)
        
        return filepath
    
    def _save_pickle(self, experiences: List[Dict], filename: str, metadata: Dict) -> str:
        """Pickle í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        filepath = os.path.join(self.save_path, f"{filename}.pkl")
        
        data = {
            "metadata": metadata,
            "experiences": experiences
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        return filepath
    
    def _save_numpy(self, experiences: List[Dict], filename: str, metadata: Dict) -> str:
        """NumPy í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        filepath = os.path.join(self.save_path, f"{filename}.npz")
        
        # ê²½í—˜ ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        arrays_dict = {"metadata": json.dumps(metadata)}
        
        for i, experience in enumerate(experiences):
            for key, value in experience.items():
                array_key = f"exp_{i}_{key}"
                if isinstance(value, np.ndarray):
                    arrays_dict[array_key] = value
                elif isinstance(value, (int, float)):
                    arrays_dict[array_key] = np.array([value])
                else:
                    arrays_dict[array_key] = np.array([str(value)])
        
        if self.compress:
            np.savez_compressed(filepath, **arrays_dict)
        else:
            np.savez(filepath, **arrays_dict)
        
        return filepath
    
    def _load_hdf5(self, filepath: str) -> Tuple[List[Dict], Dict]:
        """HDF5 í˜•ì‹ì—ì„œ ë¡œë“œ"""
        with self.h5py.File(filepath, 'r') as f:
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata = dict(f["metadata"].attrs)
            
            # ê²½í—˜ ë°ì´í„° ë¡œë“œ
            experiences = []
            exp_group = f["experiences"]
            
            for exp_key in exp_group.keys():
                exp_subgroup = exp_group[exp_key]
                experience = {}
                
                # ë°ì´í„°ì…‹ ë¡œë“œ
                for dataset_key in exp_subgroup.keys():
                    experience[dataset_key] = exp_subgroup[dataset_key][...]
                
                # ì†ì„± ë¡œë“œ
                for attr_key in exp_subgroup.attrs.keys():
                    experience[attr_key] = exp_subgroup.attrs[attr_key]
                
                experiences.append(experience)
        
        return experiences, metadata
    
    def _load_pickle(self, filepath: str) -> Tuple[List[Dict], Dict]:
        """Pickle í˜•ì‹ì—ì„œ ë¡œë“œ"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        
        return data["experiences"], data["metadata"]
    
    def _load_numpy(self, filepath: str) -> Tuple[List[Dict], Dict]:
        """NumPy í˜•ì‹ì—ì„œ ë¡œë“œ"""
        data = np.load(filepath)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata = json.loads(str(data["metadata"]))
        
        # ê²½í—˜ ë°ì´í„° ë³µì›
        experiences = []
        exp_dict = {}
        
        for key in data.keys():
            if key.startswith("exp_"):
                parts = key.split("_", 2)  # "exp", index, field_name
                exp_idx = int(parts[1])
                field_name = parts[2]
                
                if exp_idx not in exp_dict:
                    exp_dict[exp_idx] = {}
                
                value = data[key]
                if value.shape == (1,):
                    exp_dict[exp_idx][field_name] = value[0]
                else:
                    exp_dict[exp_idx][field_name] = value
        
        # ì¸ë±ìŠ¤ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        for i in sorted(exp_dict.keys()):
            experiences.append(exp_dict[i])
        
        return experiences, metadata
    
    def _detect_file_format(self, filepath: str) -> str:
        """íŒŒì¼ í™•ì¥ìë¡œ í˜•ì‹ ê°ì§€"""
        ext = os.path.splitext(filepath)[1].lower()
        if ext == ".h5":
            return "hdf5"
        elif ext == ".pkl":
            return "pickle"
        elif ext == ".npz":
            return "numpy"
        else:
            return "unknown"
    
    def _get_latest_file(self) -> str:
        """ê°€ì¥ ìµœê·¼ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        files = self.list_experience_files()
        if not files:
            return ""
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ íŒŒì¼ ë°˜í™˜
        latest_file = max(files, key=lambda x: x["modified_time"])
        return latest_file["filepath"]
    
    def _find_file_by_step(self, step_number: int) -> str:
        """íŠ¹ì • ìŠ¤í… ë²ˆí˜¸ì˜ íŒŒì¼ ì°¾ê¸°"""
        files = self.list_experience_files()
        for file_info in files:
            if file_info["step_number"] == step_number:
                return file_info["filepath"]
        return ""
    
    def _cleanup_old_files(self):
        """ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬"""
        files = self.list_experience_files()
        if len(files) <= self.max_files:
            return
        
        # ìƒì„± ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        files.sort(key=lambda x: x["created_time"])
        
        # ì˜¤ë˜ëœ íŒŒì¼ë“¤ ì‚­ì œ
        files_to_delete = files[:-self.max_files]
        for file_info in files_to_delete:
            try:
                os.remove(file_info["filepath"])
                self.logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ê²½í—˜ íŒŒì¼ ì‚­ì œ: {file_info['filename']}")
            except Exception as e:
                self.logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {file_info['filepath']}: {e}")
    
    def _get_file_size(self, filepath: str) -> str:
        """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë°˜í™˜"""
        try:
            size = os.path.getsize(filepath)
            if size < 1024:
                return f"{size} B"
            elif size < 1024 * 1024:
                return f"{size/1024:.1f} KB"
            elif size < 1024 * 1024 * 1024:
                return f"{size/(1024*1024):.1f} MB"
            else:
                return f"{size/(1024*1024*1024):.1f} GB"
        except:
            return "Unknown"
    
    def get_statistics(self) -> Dict[str, Any]:
        """ê²½í—˜ ë²„í¼ íŒŒì¼ë“¤ì˜ í†µê³„ ë°˜í™˜"""
        files = self.list_experience_files()
        
        if not files:
            return {"total_files": 0, "total_size_mb": 0}
        
        total_size = sum(f["size"] for f in files)
        total_experiences = 0
        
        # ê° íŒŒì¼ì˜ ê²½í—˜ ê°œìˆ˜ í•©ê³„ (ë©”íƒ€ë°ì´í„°ì—ì„œ ì½ê¸°)
        for file_info in files:
            try:
                _, metadata = self.load_experience_buffer(file_info["filepath"])
                total_experiences += metadata.get("num_experiences", 0)
            except:
                pass
        
        return {
            "total_files": len(files),
            "total_size_mb": total_size / (1024 * 1024),
            "total_experiences": total_experiences,
            "avg_experiences_per_file": total_experiences / len(files) if files else 0,
            "oldest_file": min(files, key=lambda x: x["created_time"])["filename"] if files else "",
            "newest_file": max(files, key=lambda x: x["created_time"])["filename"] if files else ""
        }
