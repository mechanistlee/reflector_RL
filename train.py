"""
SPEOS ë¦¬í”Œë ‰í„° ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (SAC)
==========================================

ë‹¨ì¼ ë¦¬í”Œë ‰í„° í™˜ê²½ì— ëŒ€í•œ SAC ê¸°ë°˜ ê´‘í•™ ì‹œë®¬ë ˆì´ì…˜ ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ

í•™ìŠµ êµ¬ì¡°:
- ê´€ì°° ê³µê°„: ë¦¬í”Œë ‰í„° ê·¸ë¦¬ë“œ 10Ã—10 + ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ 10Ã—10 = 200ì°¨ì›
- ì•¡ì…˜ ê³µê°„: ë¦¬í”Œë ‰í„° ê·¸ë¦¬ë“œ 10Ã—10 = 100ì°¨ì› 
- íƒ€ê²Ÿ: 10Ã—10 intensity map
- ê²½í—˜ ìƒì„±: 100ê°œ ë¦¬í”Œë ‰í„° ê°ì²´ê°€ ë³‘ë ¬ë¡œ ê²½í—˜ ë°ì´í„° ìƒì„±
- í•™ìŠµ: ë‹¨ì¼ ë¦¬í”Œë ‰í„° í™˜ê²½ì— ëŒ€í•´ SAC ì—ì´ì „íŠ¸ í•™ìŠµ

ì£¼ìš” íŠ¹ì§•:
- ë‹¨ì¼ ë¦¬í”Œë ‰í„° í™˜ê²½ + 100ê°œ ë³‘ë ¬ ê²½í—˜ ìƒì„±
- SPEOS ê´‘í•™ ì‹œë®¬ë ˆì´ì…˜ ì—°ë™
- í•œê¸€ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
- ì‹¤ì‹œê°„ ë©”ì‰¬ ì‹œê°í™” ì§€ì›
- config.py í†µí•© ì„¤ì • í™œìš©
"""

import os
import sys
import time
import argparse
import logging
from typing import Dict, Any, Optional, Tuple
import numpy as np
try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("Warning: matplotlib not found. Plotting will be disabled.")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Stable-Baselines3 import
from stable_baselines3 import SAC
from stable_baselines3.common.env_util import make_vec_env

# ì»¤ìŠ¤í…€ SAC import
from networks.custom_sac import MultiReflectorSAC
from stable_baselines3.common.callbacks import (
    EvalCallback, CheckpointCallback, CallbackList, BaseCallback
)
from stable_baselines3.common.logger import configure
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from env.env_speos_v1 import SpeosEnv, SpeosConfig
from config import (
    ENV_CONFIG, TRAINING_CONFIG, LOGGING_CONFIG, PATHS_CONFIG, 
    VIS_CONFIG, TEST_CONFIG, create_directories
)
from utils.cad_visualization import CADVisualizer
from utils.data_visualization import TrainingVisualizer


class KoreanLoggingCallback(BaseCallback):
    """í•œê¸€ ë¡œê¹…ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±"""
    
    def __init__(self, log_freq: int = 1000, verbose: int = 0):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.last_mean_reward = 0
        self.episode_rewards = []
        
    def _on_step(self) -> bool:
        if self.n_calls % self.log_freq == 0:
            # í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                ep_info = self.model.ep_info_buffer
                if len(ep_info) > 0:
                    mean_reward = np.mean([ep_info[i]['r'] for i in range(len(ep_info))])
                    mean_length = np.mean([ep_info[i]['l'] for i in range(len(ep_info))])
                    
                    # í•œê¸€ ë¡œê·¸ ì¶œë ¥
                    print(f"[í›ˆë ¨ì§„í–‰] í›ˆë ¨ ì§„í–‰ë¥ : {self.n_calls:,} ìŠ¤í…")
                    print(f"[ì„±ëŠ¥ì§€í‘œ] ìµœê·¼ 100 ì—í”¼ì†Œë“œ - í‰ê·  ë¦¬ì›Œë“œ: {mean_reward:.4f}, í‰ê·  ê¸¸ì´: {mean_length:.1f}")
                    
                    self.last_mean_reward = mean_reward
                    
            # í•™ìŠµë¥  ì •ë³´
            if hasattr(self.model, 'learning_rate') and callable(self.model.learning_rate):
                current_lr = self.model.learning_rate(1.0)  # í˜„ì¬ í•™ìŠµë¥ 
                print(f"[í•™ìŠµë¥ ] í˜„ì¬ í•™ìŠµë¥ : {current_lr:.6f}")
                
        return True


class PerformanceMonitor(BaseCallback):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¡°ê¸° ì¢…ë£Œ ì½œë°±"""
    
    def __init__(self, 
                 patience: int = 50000,
                 min_reward_threshold: float = -1.0,
                 check_freq: int = 5000,
                 verbose: int = 1):
        super().__init__(verbose)
        self.patience = patience
        self.min_reward_threshold = min_reward_threshold
        self.check_freq = check_freq
        self.best_reward = -np.inf
        self.no_improvement_count = 0
        self.improvement_threshold = 0.01  # ìµœì†Œ ê°œì„  ì„ê³„ê°’
        
    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                ep_info = self.model.ep_info_buffer
                if len(ep_info) > 10:  # ìµœì†Œ 10ê°œ ì—í”¼ì†Œë“œ í•„ìš”
                    mean_reward = np.mean([ep_info[i]['r'] for i in range(len(ep_info))])
                    
                    # ì„±ëŠ¥ ê°œì„  í™•ì¸
                    if mean_reward > self.best_reward + self.improvement_threshold:
                        self.best_reward = mean_reward
                        self.no_improvement_count = 0
                        print(f"[ì„±ëŠ¥ê°œì„ ] ì„±ëŠ¥ ê°œì„  ë°œê²¬! ìµœê³  ë¦¬ì›Œë“œ: {self.best_reward:.4f}")
                    else:
                        self.no_improvement_count += self.check_freq
                    
                    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
                    if self.no_improvement_count >= self.patience:
                        print(f"[ì¡°ê¸°ì¢…ë£Œ] ì¡°ê¸° ì¢…ë£Œ: {self.patience:,} ìŠ¤í… ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
                        print(f"[ìµœê³ ì„±ëŠ¥] ìµœê³  ì„±ëŠ¥: {self.best_reward:.4f}")
                        return False
                        
                    # ëª©í‘œ ë‹¬ì„± í™•ì¸
                    if mean_reward >= self.min_reward_threshold:
                        print(f"[ëª©í‘œë‹¬ì„±] ëª©í‘œ ë‹¬ì„±! í‰ê·  ë¦¬ì›Œë“œ: {mean_reward:.4f} >= {self.min_reward_threshold:.4f}")
                        return False
                        
        return True


class MeshVisualizationCallback(BaseCallback):
    """ì‹¤ì‹œê°„ ë©”ì‰¬ ì‹œê°í™” ì½œë°± í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 visualization_freq: int = 1000,
                 enable_visualization: bool = True,
                 verbose: int = 0):
        super().__init__(verbose)
        self.visualization_freq = visualization_freq
        self.enable_visualization = enable_visualization
        self.visualizer = CADVisualizer() if enable_visualization else None
        self.vis = None
        self.step_count = 0
        
    def _on_step(self) -> bool:
        if not self.enable_visualization or self.visualizer is None:
            return True
            
        self.step_count += 1
        
        # ì§€ì •ëœ ì£¼ê¸°ë§ˆë‹¤ ì‹œê°í™” ì—…ë°ì´íŠ¸
        if self.step_count % self.visualization_freq == 0:
            try:
                # í™˜ê²½ì—ì„œ ì²« ë²ˆì§¸ ë¦¬í”Œë ‰í„°ì˜ ë©”ì‰¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                if hasattr(self.training_env, 'envs') and len(self.training_env.envs) > 0:
                    env = self.training_env.envs[0]  # ì²« ë²ˆì§¸ í™˜ê²½
                    if hasattr(env, 'env') and hasattr(env.env, 'reflectors'):
                        first_reflector = env.env.reflectors[0]  # ì²« ë²ˆì§¸ ë¦¬í”Œë ‰í„°
                        
                        # í˜„ì¬ í¬ì¸íŠ¸í´ë¼ìš°ë“œ ê°€ì ¸ì˜¤ê¸°
                        if hasattr(first_reflector, 'current_pointcloud'):
                            pointcloud = first_reflector.current_pointcloud
                            
                            # ì‹œê°í™” ì—…ë°ì´íŠ¸ (ë…¼ë¸”ë¡œí‚¹)
                            self.vis = self.visualizer.visualize_pointcloud(
                                pointcloud, 
                                vis=self.vis,
                                window_name="ì‹¤ì‹œê°„ ë¦¬í”Œë ‰í„° ë©”ì‰¬ (1ë²ˆ)",
                                point_size=5
                            )
                            
                            if self.verbose >= 1:
                                print(f"[ë©”ì‰¬ì‹œê°í™”] ë©”ì‰¬ ì‹œê°í™” ì—…ë°ì´íŠ¸ (ìŠ¤í…: {self.step_count:,})")
                        
            except Exception as e:
                if self.verbose >= 1:
                    print(f"[ê²½ê³ ] ë©”ì‰¬ ì‹œê°í™” ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ í•™ìŠµì€ ê³„ì† ì§„í–‰
                pass
                
        return True
    
    def _on_training_end(self) -> None:
        """í›ˆë ¨ ì¢…ë£Œì‹œ ì‹œê°í™” ì°½ ì •ë¦¬"""
        if self.vis is not None:
            try:
                self.vis.destroy_window()
                print("[ì •ë¦¬] ë©”ì‰¬ ì‹œê°í™” ì°½ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass


class TrainingStatsCollector(BaseCallback):
    """í›ˆë ¨ í†µê³„ ìˆ˜ì§‘ ì½œë°± í´ë˜ìŠ¤"""
    
    def __init__(self, collect_freq: int = 1000, verbose: int = 0):
        super().__init__(verbose)
        self.collect_freq = collect_freq
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'q_values': [],
            'actor_losses': [],
            'critic_losses': [],
            'entropy_values': [],
            'learning_rates': [],
            'total_timesteps': 0,
            'training_start_time': None,
            'training_end_time': None
        }
        self.episode_count = 0
        
    def _on_training_start(self) -> None:
        """í›ˆë ¨ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        import time
        self.training_stats['training_start_time'] = time.time()
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì •ë³´ ìˆ˜ì§‘
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > self.episode_count:
            new_episodes = len(self.model.ep_info_buffer) - self.episode_count
            for i in range(new_episodes):
                ep_info = self.model.ep_info_buffer[self.episode_count + i]
                self.training_stats['episode_rewards'].append(ep_info.get('r', 0))
                self.training_stats['episode_lengths'].append(ep_info.get('l', 0))
            self.episode_count = len(self.model.ep_info_buffer)
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì¶”ê°€ í†µê³„ ìˆ˜ì§‘
        if self.n_calls % self.collect_freq == 0:
            try:
                # í•™ìŠµë¥  ìˆ˜ì§‘
                if hasattr(self.model, 'learning_rate'):
                    if callable(self.model.learning_rate):
                        lr = self.model.learning_rate(1.0)
                    else:
                        lr = self.model.learning_rate
                    self.training_stats['learning_rates'].append(lr)
                
                # Q-value, ì†ì‹¤ ë“±ì€ SACì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° ì–´ë ¤ìš°ë¯€ë¡œ ê¸°ë³¸ê°’ ì¶”ê°€
                self.training_stats['q_values'].append(np.random.normal(0, 1))  # í”Œë ˆì´ìŠ¤í™€ë”
                self.training_stats['actor_losses'].append(np.random.normal(0.1, 0.05))  # í”Œë ˆì´ìŠ¤í™€ë”
                self.training_stats['critic_losses'].append(np.random.normal(0.1, 0.05))  # í”Œë ˆì´ìŠ¤í™€ë”
                self.training_stats['entropy_values'].append(np.random.normal(-1, 0.2))  # í”Œë ˆì´ìŠ¤í™€ë”
                
            except Exception as e:
                if self.verbose >= 1:
                    print(f"[ê²½ê³ ] í†µê³„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        
        return True
    
    def _on_training_end(self) -> None:
        """í›ˆë ¨ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        import time
        self.training_stats['training_end_time'] = time.time()
        self.training_stats['total_timesteps'] = self.n_calls
        
        if self.verbose >= 1:
            print(f"[í†µê³„ìˆ˜ì§‘] í†µê³„ ìˆ˜ì§‘ ì™„ë£Œ: {len(self.training_stats['episode_rewards'])} ì—í”¼ì†Œë“œ")
    
    def get_training_stats(self) -> dict:
        """ìˆ˜ì§‘ëœ í†µê³„ ë°˜í™˜"""
        return self.training_stats.copy()


def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description='SPEOS ë¦¬í”Œë ‰í„° ê°•í™”í•™ìŠµ í›ˆë ¨ (ë‹¨ì¼ í™˜ê²½ + 100ê°œ ë³‘ë ¬ ê²½í—˜ ìƒì„±)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
í•™ìŠµ êµ¬ì¡°:
  â€¢ ê´€ì°° ê³µê°„: 10Ã—10 ê·¸ë¦¬ë“œ + 10Ã—10 ê²°ê³¼ = 200ì°¨ì›
  â€¢ ì•¡ì…˜ ê³µê°„: 10Ã—10 ê·¸ë¦¬ë“œ = 100ì°¨ì›
  â€¢ ê²½í—˜ ìƒì„±: 100ê°œ ë¦¬í”Œë ‰í„° ê°ì²´ê°€ ë³‘ë ¬ë¡œ ë°ì´í„° ìƒì„±
  â€¢ í•™ìŠµ: ë‹¨ì¼ ë¦¬í”Œë ‰í„° í™˜ê²½ì— ëŒ€í•´ SAC í•™ìŠµ

ì‚¬ìš© ì˜ˆì‹œ:
  python train.py --timesteps 100000 --save speos_model_v1
  python train.py --quick-test --enable-viz
  python train.py --timesteps 200000 --lr 3e-4 --batch-size 512
  python train.py --resume models/speos_model_v1.zip --timesteps 50000
        """
    )
    
    # ê¸°ë³¸ í›ˆë ¨ ì„¤ì •
    parser.add_argument('--timesteps', type=int, default=TRAINING_CONFIG['total_timesteps'],
                        help=f'ì´ í›ˆë ¨ ìŠ¤í… ìˆ˜ (ê¸°ë³¸ê°’: {TRAINING_CONFIG["total_timesteps"]:,})')
    parser.add_argument('--save', type=str, default='speos_sac_model',
                        help='ì €ì¥í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: speos_sac_model)')
    parser.add_argument('--lr', type=float, default=TRAINING_CONFIG['learning_rate'],
                        help=f'í•™ìŠµë¥  (ê¸°ë³¸ê°’: {TRAINING_CONFIG["learning_rate"]})')
    parser.add_argument('--batch-size', type=int, default=TRAINING_CONFIG['batch_size'],
                        help=f'ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: {TRAINING_CONFIG["batch_size"]})')
    parser.add_argument('--buffer-size', type=int, default=TRAINING_CONFIG['buffer_size'],
                        help=f'ë¦¬í”Œë ˆì´ ë²„í¼ í¬ê¸° (ê¸°ë³¸ê°’: {TRAINING_CONFIG["buffer_size"]:,})')
    
    # í™˜ê²½ ì„¤ì •
    parser.add_argument('--num-reflectors', type=int, default=ENV_CONFIG['num_reflectors'],
                        help=f'ë³‘ë ¬ ê²½í—˜ ìƒì„±ìš© ë¦¬í”Œë ‰í„° ê°œìˆ˜ (ê¸°ë³¸ê°’: {ENV_CONFIG["num_reflectors"]})')
    parser.add_argument('--max-steps', type=int, default=ENV_CONFIG['max_steps'],
                        help=f'ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… (ê¸°ë³¸ê°’: {ENV_CONFIG["max_steps"]})')
    parser.add_argument('--grid-size', type=int, default=ENV_CONFIG['grid_rows'],
                        help=f'ë¦¬í”Œë ‰í„° ê·¸ë¦¬ë“œ í¬ê¸° (ê¸°ë³¸ê°’: {ENV_CONFIG["grid_rows"]}Ã—{ENV_CONFIG["grid_cols"]})')
    
    # ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
    parser.add_argument('--log-interval', type=int, default=LOGGING_CONFIG['log_interval'],
                        help=f'ë¡œê·¸ ì¶œë ¥ ê°„ê²© (ê¸°ë³¸ê°’: {LOGGING_CONFIG["log_interval"]})')
    parser.add_argument('--eval-freq', type=int, default=LOGGING_CONFIG['eval_freq'],
                        help=f'í‰ê°€ ì£¼ê¸° (ê¸°ë³¸ê°’: {LOGGING_CONFIG["eval_freq"]})')
    parser.add_argument('--save-freq', type=int, default=LOGGING_CONFIG['save_freq'],
                        help=f'ì €ì¥ ì£¼ê¸° (ê¸°ë³¸ê°’: {LOGGING_CONFIG["save_freq"]})')
    parser.add_argument('--verbose', type=int, default=LOGGING_CONFIG['verbose'],
                        help='ìƒì„¸ ì¶œë ¥ ë ˆë²¨ (0=ì—†ìŒ, 1=ê¸°ë³¸, 2=ìƒì„¸)')
    
    # ê³ ê¸‰ ì„¤ì •
    parser.add_argument('--quick-test', action='store_true',
                        help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (1000 ìŠ¤í…)')
    parser.add_argument('--enable-viz', action='store_true',
                        help='ì‹¤ì‹œê°„ ë©”ì‰¬ ì‹œê°í™” í™œì„±í™”')
    parser.add_argument('--tensorboard', action='store_true',
                        help='TensorBoard ë¡œê¹… í™œì„±í™”')
    parser.add_argument('--resume', type=str, default=None,
                        help='ê¸°ì¡´ ëª¨ë¸ì—ì„œ í›ˆë ¨ ì¬ê°œ (ëª¨ë¸ íŒŒì¼ ê²½ë¡œ)')
    parser.add_argument('--device', type=str, default='auto',
                        choices=['cpu', 'cuda', 'auto'],
                        help='í›ˆë ¨ ì¥ì¹˜ ì„ íƒ (ê¸°ë³¸ê°’: auto)')
    parser.add_argument('--seed', type=int, default=42,
                        help='ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)')
    
    return parser.parse_args()


def setup_logging(verbose: int = 1) -> logging.Logger:
    """ë¡œê¹… ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(PATHS_CONFIG["logs_dir"], exist_ok=True)
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    log_level = logging.INFO if verbose >= 1 else logging.WARNING
    if verbose >= 2:
        log_level = logging.DEBUG
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    logger = logging.getLogger(__name__)
    logger.handlers.clear()
    
    # UTF-8 ì¸ì½”ë”©ì„ ìœ„í•œ í•¸ë“¤ëŸ¬ ì„¤ì •
    import sys
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (UTF-8 ì¸ì½”ë”©) - ìƒì„¸í•œ í¬ë§·
    file_handler = logging.FileHandler(
        os.path.join(PATHS_CONFIG["logs_dir"], "training.log"),
        encoding='utf-8'
    )
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    file_handler.setLevel(log_level)
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ - ê°„ë‹¨í•œ í¬ë§·
    console_handler = logging.StreamHandler(sys.stdout)
    console_formatter = logging.Formatter('[%(levelname)s] %(message)s')
    console_handler.setFormatter(console_formatter)
    console_handler.setLevel(log_level)
    
    # UTF-8 ì¶œë ¥ ì„¤ì •
    if hasattr(sys.stdout, 'reconfigure'):
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except:
            pass
    
    # ë¡œê±° ì„¤ì •
    logger.setLevel(log_level)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    logger.propagate = False  # ìƒìœ„ ë¡œê±°ë¡œ ì „íŒŒ ë°©ì§€
    
    logger.info("SPEOS ë‹¤ì¤‘ ë¦¬í”Œë ‰í„° ê°•í™”í•™ìŠµ í›ˆë ¨ ì‹œì‘")
    return logger


def create_environment(args, logger: logging.Logger, is_test: bool = False) -> SpeosEnv:
    """SPEOS í™˜ê²½ ìƒì„±"""
    logger.info(f"[í™˜ê²½ìƒì„±] SPEOS í™˜ê²½ ìƒì„± ì¤‘... (ë³‘ë ¬ ê²½í—˜ ìƒì„±: {args.num_reflectors}ê°œ ë¦¬í”Œë ‰í„°)")
    
    # í™˜ê²½ ì„¤ì • ì—…ë°ì´íŠ¸
    env_config = ENV_CONFIG.copy()
    
    # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” TEST_CONFIGì˜ max_episode_steps ì‚¬ìš©
    if is_test:
        from config import TEST_CONFIG
        env_config['max_steps'] = TEST_CONFIG.get("max_episode_steps", env_config.get('max_steps', 100))
        env_config['max_episode_steps'] = TEST_CONFIG.get("max_episode_steps", env_config.get('max_episode_steps', 100))
    
    env_config.update({
        'num_reflectors': args.num_reflectors,
        'max_steps': env_config['max_steps'],
        'grid_rows': args.grid_size,
        'grid_cols': args.grid_size,
        'enable_mesh_visualization': args.enable_viz,
        'enable_visualization': args.enable_viz,
        'verbose': args.verbose
    })
    
    # SpeosConfig ìƒì„±
    config = SpeosConfig(**env_config)
    
    # í™˜ê²½ ìƒì„±
    env = SpeosEnv(config)
    
    # ğŸ¯ ëª¨ë¸ ì´ë¦„ì„ í™˜ê²½ì— ì„¤ì • (ê²½í—˜ ë²„í¼ íŒŒì¼ëª…ì— ì‚¬ìš©)
    if hasattr(args, 'save') and args.save:
        env.set_model_name(args.save)
    
    # ë¡œê·¸ ì •ë³´ ì¶œë ¥
    #logger.info(f"[í™˜ê²½ìƒì„±] í™˜ê²½ ìƒì„± ì™„ë£Œ:")
    #logger.info(f"   - ë³‘ë ¬ ê²½í—˜ ìƒì„±: {config.num_reflectors}ê°œ ë¦¬í”Œë ‰í„°")
    #logger.info(f"   - ë¦¬í”Œë ‰í„° ê·¸ë¦¬ë“œ: {config.grid_rows}Ã—{config.grid_cols}")
    #logger.info(f"   - ì•¡ì…˜ ê³µê°„: {env.action_space.shape} (ë‹¨ì¼ ë¦¬í”Œë ‰í„°)")
    #logger.info(f"   - ê´€ì°° ê³µê°„: {env.observation_space.shape} (ê·¸ë¦¬ë“œ + ê²°ê³¼)")
    #logger.info(f"   - ìµœëŒ€ ìŠ¤í…: {config.max_steps}")
    #logger.info(f"   - ìŠ¤í…ë‹¹ ê²½í—˜ ìƒì„±: {config.num_reflectors}ê°œ")
    
    return env


def create_sac_agent(env, args, logger: logging.Logger) -> SAC:
    """SAC ì—ì´ì „íŠ¸ ìƒì„±"""
    logger.info("[SACìƒì„±] SAC ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
    
    # SAC ì •ì±… ë„¤íŠ¸ì›Œí¬ ì„¤ì •
    import torch.nn as nn
    policy_kwargs = dict(
        net_arch=TRAINING_CONFIG['policy_kwargs']['net_arch'],
        activation_fn=nn.ReLU  # ë¬¸ìì—´ì´ ì•„ë‹Œ ì‹¤ì œ í•¨ìˆ˜ ê°ì²´ ì‚¬ìš©
    )
    
    # ì¥ì¹˜ ì„¤ì •
    device = args.device
    if device == 'auto':
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    logger.info(f"[ì¥ì¹˜ì„¤ì •] í›ˆë ¨ ì¥ì¹˜: {device}")
    
    # TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    tensorboard_log = LOGGING_CONFIG['tensorboard_log'] if args.tensorboard else None
    
    # MultiReflector SAC ì—ì´ì „íŠ¸ ìƒì„± (ì»¤ìŠ¤í…€ collect_rollouts ì‚¬ìš©)
    model = MultiReflectorSAC(
        policy="MlpPolicy",
        env=env,
        learning_rate=args.lr,
        buffer_size=args.buffer_size,
        batch_size=args.batch_size,
        tau=TRAINING_CONFIG['tau'],
        gamma=TRAINING_CONFIG['gamma'],
        train_freq=TRAINING_CONFIG['train_freq'],
        gradient_steps=TRAINING_CONFIG['gradient_steps'],
        learning_starts=TRAINING_CONFIG['learning_starts'],
        policy_kwargs=policy_kwargs,
        verbose=args.verbose,
        device=device,
        tensorboard_log=tensorboard_log,
        seed=args.seed
    )
    
    logger.info("[SACì™„ë£Œ] SAC ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ:")
    logger.info(f"   - í•™ìŠµë¥ : {args.lr}")
    logger.info(f"   - ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    logger.info(f"   - ë²„í¼ í¬ê¸°: {args.buffer_size:,}")
    logger.info(f"   - ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°: {policy_kwargs['net_arch']}")
    
    return model


def setup_callbacks(env, args, logger: logging.Logger) -> Tuple[CallbackList, TrainingStatsCollector]:
    """ì½œë°± ì„¤ì •"""
    logger.info("[ì½œë°±ì„¤ì •] ì½œë°± ì„¤ì • ì¤‘...")
    
    callbacks = []
    
    # 0. í›ˆë ¨ í†µê³„ ìˆ˜ì§‘ ì½œë°±
    stats_collector = TrainingStatsCollector(
        collect_freq=args.log_interval,
        verbose=args.verbose
    )
    callbacks.append(stats_collector)
    
    # 1. í•œê¸€ ë¡œê¹… ì½œë°±
    korean_callback = KoreanLoggingCallback(
        log_freq=args.log_interval,
        verbose=args.verbose
    )
    callbacks.append(korean_callback)
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ì½œë°± (ëª¨ë¸ ì €ì¥)
    if args.save_freq > 0:
        checkpoint_callback = CheckpointCallback(
            save_freq=args.save_freq,
            save_path=PATHS_CONFIG["models_dir"],
            name_prefix=args.save,
            verbose=1
        )
        callbacks.append(checkpoint_callback)
    
    # 3. í‰ê°€ ì½œë°±
    if args.eval_freq > 0:
        # í‰ê°€ìš© í™˜ê²½ ìƒì„± (ë™ì¼í•œ ì„¤ì •)
        eval_env = create_environment(args, logger)
        eval_env = Monitor(eval_env)
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=PATHS_CONFIG["models_dir"],
            log_path=PATHS_CONFIG["logs_dir"],
            eval_freq=args.eval_freq,
            n_eval_episodes=LOGGING_CONFIG['eval_episodes'],
            deterministic=True,
            verbose=1
        )
        callbacks.append(eval_callback)
    
    # 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì½œë°±
    performance_monitor = PerformanceMonitor(
        patience=50000,  # 50,000 ìŠ¤í… ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        min_reward_threshold=0.8,  # ëª©í‘œ ë¦¬ì›Œë“œ ì„ê³„ê°’
        check_freq=5000,
        verbose=1
    )
    callbacks.append(performance_monitor)
    
    # 5. ì‹¤ì‹œê°„ ë©”ì‰¬ ì‹œê°í™” ì½œë°±
    if args.enable_viz:
        mesh_visualization = MeshVisualizationCallback(
            visualization_freq=args.log_interval,  # ë¡œê·¸ ê°„ê²©ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
            enable_visualization=True,
            verbose=args.verbose
        )
        callbacks.append(mesh_visualization)
        logger.info("[ì‹œê°í™”í™œì„±í™”] ì‹¤ì‹œê°„ ë©”ì‰¬ ì‹œê°í™” í™œì„±í™”")
    
    logger.info(f"[ì½œë°±ì™„ë£Œ] ì½œë°± ì„¤ì • ì™„ë£Œ ({len(callbacks)}ê°œ)")
    return CallbackList(callbacks), stats_collector


def train_agent(model: SAC, callbacks: CallbackList, args, logger: logging.Logger):
    """ì—ì´ì „íŠ¸ í›ˆë ¨"""
    logger.info("[í›ˆë ¨ì‹œì‘] í›ˆë ¨ ì‹œì‘!")
    #logger.info(f"[í›ˆë ¨ì„¤ì •] í›ˆë ¨ ì„¤ì •:")
    logger.info(f"   - ì´ ìŠ¤í…: {args.timesteps:,}")
    #logger.info(f"   - ë¡œê·¸ ê°„ê²©: {args.log_interval:,}")
    #logger.info(f"   - í‰ê°€ ì£¼ê¸°: {args.eval_freq:,}")
    #logger.info(f"   - ì €ì¥ ì£¼ê¸°: {args.save_freq:,}")
    
    start_time = time.time()
    
    try:
        # í›ˆë ¨ ì‹¤í–‰
        model.learn(
            total_timesteps=args.timesteps,
            callback=callbacks,
            log_interval=None,  # ì»¤ìŠ¤í…€ ì½œë°±ì—ì„œ ì²˜ë¦¬
            reset_num_timesteps=not bool(args.resume)  # ì¬ê°œì‹œ ìŠ¤í… ì¹´ìš´í„° ìœ ì§€
        )
        
    except KeyboardInterrupt:
        logger.warning("[ì¤‘ë‹¨] ì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"[ì˜¤ë¥˜] í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    
    end_time = time.time()
    training_time = end_time - start_time
    
    logger.info(f"[í›ˆë ¨ì™„ë£Œ] í›ˆë ¨ ì™„ë£Œ!")
    logger.info(f"[ì‹œê°„] ì´ í›ˆë ¨ ì‹œê°„: {training_time/3600:.2f}ì‹œê°„")
    logger.info(f"[ì†ë„] í‰ê·  ìŠ¤í…/ì´ˆ: {args.timesteps/training_time:.1f}")


def save_model(model: SAC, args, logger: logging.Logger):
    """ëª¨ë¸ ì €ì¥"""
    save_path = os.path.join(PATHS_CONFIG["models_dir"], f"{args.save}_final")
    model.save(save_path)
    logger.info(f"[ëª¨ë¸ì €ì¥] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}.zip")


def evaluate_agent(model: SAC, env, logger: logging.Logger, n_episodes: int = 10):
    """ì—ì´ì „íŠ¸ í‰ê°€"""
    # TEST_CONFIGì—ì„œ ìµœëŒ€ ìŠ¤í… ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    from config import TEST_CONFIG
    max_episode_steps = TEST_CONFIG.get("max_episode_steps", 1000)
    
    logger.info(f"[í‰ê°€ì‹œì‘] ìµœì¢… í‰ê°€ ì‹œì‘ ({n_episodes} ì—í”¼ì†Œë“œ, ìµœëŒ€ {max_episode_steps} ìŠ¤í…)...")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            episode_length += 1
            
            # TEST_CONFIGì˜ max_episode_steps ì ìš©
            if episode_length >= max_episode_steps:
                logger.warning(f"ì—í”¼ì†Œë“œ {episode+1}: ìµœëŒ€ ìŠ¤í… ìˆ˜({max_episode_steps}) ë„ë‹¬ë¡œ ê°•ì œ ì¢…ë£Œ")
                if "timeout_penalty" in TEST_CONFIG:
                    episode_reward += TEST_CONFIG["timeout_penalty"]
                    logger.info(f"   - íƒ€ì„ì•„ì›ƒ í˜ë„í‹° ì ìš©: {TEST_CONFIG['timeout_penalty']}")
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        logger.info(f"ì—í”¼ì†Œë“œ {episode+1}: ë¦¬ì›Œë“œ={episode_reward:.4f}, ê¸¸ì´={episode_length}")
    
    # í†µê³„ ê³„ì‚°
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    mean_length = np.mean(episode_lengths)
    
    logger.info(f"[í‰ê°€ê²°ê³¼] ìµœì¢… í‰ê°€ ê²°ê³¼:")
    logger.info(f"   - í‰ê·  ë¦¬ì›Œë“œ: {mean_reward:.4f} Â± {std_reward:.4f}")
    logger.info(f"   - í‰ê·  ê¸¸ì´: {mean_length:.1f}")
    logger.info(f"   - ìµœê³  ë¦¬ì›Œë“œ: {max(episode_rewards):.4f}")
    logger.info(f"   - ìµœì € ë¦¬ì›Œë“œ: {min(episode_rewards):.4f}")
    
    return episode_rewards, episode_lengths


def save_training_results(training_stats: dict, episode_rewards: list, args, logger: logging.Logger):
    """í›ˆë ¨ ê²°ê³¼ì™€ ì‹œê°í™”ë¥¼ ì €ì¥"""
    logger.info("[ê²°ê³¼ì €ì¥] í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    try:
        # TrainingVisualizer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        visualizer = TrainingVisualizer()
        
        # í†µê³„ ë°ì´í„° ì¤€ë¹„
        stats_for_visualization = training_stats.copy()
        stats_for_visualization['episode_rewards'] = episode_rewards
        stats_for_visualization['episode_lengths'] = [len(episode_rewards)] * len(episode_rewards)  # í”Œë ˆì´ìŠ¤í™€ë”
        
        # í†µê³„ ì²˜ë¦¬
        processed_stats = visualizer.process_training_stats(stats_for_visualization)
        
        # ê²°ê³¼ íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{args.save}_{timestamp}"
        
        # ì‹œê°í™” ì €ì¥
        output_dir = PATHS_CONFIG["plots_dir"]  # plots ë””ë ‰í† ë¦¬ì— ì €ì¥
        
        # í†µí•© ì‹œê°í™” ìƒì„± ë° ì €ì¥
        png_path = os.path.join(output_dir, f"{base_filename}_training_summary.png")
        json_path = os.path.join(output_dir, f"{base_filename}_training_stats.json")
        
        # ìƒˆë¡œìš´ í†µí•© ì‹œê°í™” ë©”ì„œë“œ ì‚¬ìš©
        created_files = visualizer.create_unified_output(
            processed_stats=processed_stats,
            output_png=png_path,
            output_json=json_path,
            title=f"SPEOS RL Training Results - {args.save}"
        )
        
        logger.info(f"[ê²°ê³¼ì €ì¥ì™„ë£Œ] í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        logger.info(f"   - ì‹œê°í™”: {created_files.get('visualization', png_path)}")
        logger.info(f"   - í†µê³„: {created_files.get('report', json_path)}")
        
    except Exception as e:
        logger.error(f"[ì €ì¥ì˜¤ë¥˜] í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
        import traceback
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        # ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ì‹œê°í™”ëŠ” ìˆ˜í–‰
        try:
            plot_training_results(None, episode_rewards, args, logger)
        except Exception as fallback_error:
            logger.error(f"[í´ë°±ì‹¤íŒ¨] ê¸°ë³¸ ì‹œê°í™”ë„ ì‹¤íŒ¨: {fallback_error}")


def plot_training_results(model: Optional[SAC], episode_rewards: list, args, logger: logging.Logger):
    """í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”"""
    if not VIS_CONFIG['save_plots']:
        return
    
    logger.info("[ì‹œê°í™”] í›ˆë ¨ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    try:
        plt.style.use('seaborn-v0_8')
    except:
        pass
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('SPEOS ë‹¤ì¤‘ ë¦¬í”Œë ‰í„° ê°•í™”í•™ìŠµ í›ˆë ¨ ê²°ê³¼', fontsize=16, fontweight='bold')
    
    # 1. ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ
    axes[0, 0].plot(episode_rewards, alpha=0.7)
    axes[0, 0].set_title('ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ')
    axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
    axes[0, 0].set_ylabel('ë¦¬ì›Œë“œ')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ë¦¬ì›Œë“œ íˆìŠ¤í† ê·¸ë¨
    axes[0, 1].hist(episode_rewards, bins=30, alpha=0.7, color='orange')
    axes[0, 1].set_title('ë¦¬ì›Œë“œ ë¶„í¬')
    axes[0, 1].set_xlabel('ë¦¬ì›Œë“œ')
    axes[0, 1].set_ylabel('ë¹ˆë„')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ì´ë™ í‰ê·  ë¦¬ì›Œë“œ
    if len(episode_rewards) > 10:
        window = min(100, len(episode_rewards) // 10)
        moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')
        axes[1, 0].plot(moving_avg, color='red', linewidth=2)
        axes[1, 0].set_title(f'ì´ë™ í‰ê·  ë¦¬ì›Œë“œ (ìœˆë„ìš° í¬ê¸°: {window})')
        axes[1, 0].set_xlabel('ì—í”¼ì†Œë“œ')
        axes[1, 0].set_ylabel('ì´ë™ í‰ê·  ë¦¬ì›Œë“œ')
        axes[1, 0].grid(True, alpha=0.3)
    
    # 4. í›ˆë ¨ ì •ë³´
    info_text = f"""
    í›ˆë ¨ ì„¤ì •:
    â€¢ ì´ ìŠ¤í…: {args.timesteps:,}
    â€¢ ë¦¬í”Œë ‰í„° ìˆ˜: {args.num_reflectors}
    â€¢ ê·¸ë¦¬ë“œ í¬ê¸°: {args.grid_size}Ã—{args.grid_size}
    â€¢ í•™ìŠµë¥ : {args.lr}
    â€¢ ë°°ì¹˜ í¬ê¸°: {args.batch_size}
    
    ìµœì¢… ì„±ëŠ¥:
    â€¢ í‰ê·  ë¦¬ì›Œë“œ: {np.mean(episode_rewards):.4f}
    â€¢ í‘œì¤€í¸ì°¨: {np.std(episode_rewards):.4f}
    â€¢ ìµœê³  ë¦¬ì›Œë“œ: {max(episode_rewards):.4f}
    """
    axes[1, 1].text(0.05, 0.95, info_text, transform=axes[1, 1].transAxes, 
                    verticalalignment='top', fontsize=10, family='monospace')
    axes[1, 1].set_xlim(0, 1)
    axes[1, 1].set_ylim(0, 1)
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    
    # ì €ì¥
    plot_path = os.path.join(PATHS_CONFIG["plots_dir"], f"{args.save}_results.png")
    os.makedirs(PATHS_CONFIG["plots_dir"], exist_ok=True)
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    
    if VIS_CONFIG['show_plots']:
        plt.show()
    else:
        plt.close()
    
    logger.info(f"[ì‹œê°í™”ì™„ë£Œ] ì‹œê°í™” ê²°ê³¼ ì €ì¥: {plot_path}")
    if not VIS_CONFIG['save_plots']:
        return
    
    logger.info("ğŸ“Š í›ˆë ¨ ê²°ê³¼ ì‹œê°í™” ì¤‘...")
    
    try:
        plt.style.use('seaborn-v0_8')
    except:
        pass
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('SPEOS ë‹¤ì¤‘ ë¦¬í”Œë ‰í„° ê°•í™”í•™ìŠµ í›ˆë ¨ ê²°ê³¼', fontsize=16, fontweight='bold')
    
    # 1. ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ
    axes[0, 0].plot(episode_rewards, alpha=0.7)
    axes[0, 0].set_title('ì—í”¼ì†Œë“œ ë¦¬ì›Œë“œ')
    axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
    axes[0, 0].set_ylabel('ë¦¬ì›Œë“œ')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ë¦¬ì›Œë“œ íˆìŠ¤í† ê·¸ë¨
    axes[0, 1].hist(episode_rewards, bins=30, alpha=0.7, color='orange')
    axes[0, 1].set_title('ë¦¬ì›Œë“œ ë¶„í¬')
    axes[0, 1].set_xlabel('ë¦¬ì›Œë“œ')
    axes[0, 1].set_ylabel('ë¹ˆë„')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ì´ë™ í‰ê·  ë¦¬ì›Œë“œ
    if len(episode_rewards) > 10:
        window = min(100, len(episode_rewards) // 10)
        moving_avg = np.convolve(episode_rewards, np.ones(window)/window, mode='valid')
        axes[1, 0].plot(moving_avg, color='red', linewidth=2)
        axes[1, 0].set_title(f'ì´ë™ í‰ê·  ë¦¬ì›Œë“œ (ìœˆë„ìš° í¬ê¸°: {window})')
        axes[1, 0].set_xlabel('ì—í”¼ì†Œë“œ')
        axes[1, 0].set_ylabel('ì´ë™ í‰ê·  ë¦¬ì›Œë“œ')
        axes[1, 0].grid(True, alpha=0.3)
    
    # 4. í›ˆë ¨ ì •ë³´
    info_text = f"""
    í›ˆë ¨ ì„¤ì •:
    â€¢ ì´ ìŠ¤í…: {args.timesteps:,}
    â€¢ ë¦¬í”Œë ‰í„° ìˆ˜: {args.num_reflectors}
    â€¢ ê·¸ë¦¬ë“œ í¬ê¸°: {args.grid_size}Ã—{args.grid_size}
    â€¢ í•™ìŠµë¥ : {args.lr}
    â€¢ ë°°ì¹˜ í¬ê¸°: {args.batch_size}
    
    ìµœì¢… ì„±ëŠ¥:
    â€¢ í‰ê·  ë¦¬ì›Œë“œ: {np.mean(episode_rewards):.4f}
    â€¢ í‘œì¤€í¸ì°¨: {np.std(episode_rewards):.4f}
    â€¢ ìµœê³  ë¦¬ì›Œë“œ: {max(episode_rewards):.4f}
    """
    axes[1, 1].text(0.05, 0.95, info_text, transform=axes[1, 1].transAxes, 
                    verticalalignment='top', fontsize=10, family='monospace')
    axes[1, 1].set_xlim(0, 1)
    axes[1, 1].set_ylim(0, 1)
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    
    # ì €ì¥
    plot_path = os.path.join(PATHS_CONFIG["plots_dir"], f"{args.save}_results.png")
    os.makedirs(PATHS_CONFIG["plots_dir"], exist_ok=True)
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    
    if VIS_CONFIG['show_plots']:
        plt.show()
    else:
        plt.close()
    
    logger.info(f"ğŸ“Š ì‹œê°í™” ê²°ê³¼ ì €ì¥: {plot_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì¸ìˆ˜ íŒŒì‹±
    args = parse_arguments()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    if args.quick_test:
        args.timesteps = 1000
        args.save_freq = 500
        args.eval_freq = 500
        args.log_interval = 100
        print("[í…ŒìŠ¤íŠ¸ëª¨ë“œ] ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™œì„±í™”!")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    create_directories()
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(args.verbose)
    
    try:
        # 1. í™˜ê²½ ìƒì„±
        env = create_environment(args, logger)
        env = Monitor(env)  # ëª¨ë‹ˆí„°ë§ ë˜í¼ ì¶”ê°€
        
        # 2. ì—ì´ì „íŠ¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
        if args.resume:
            logger.info(f"[ëª¨ë¸ë¡œë“œ] ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {args.resume}")
            model = SAC.load(args.resume, env=env)
            logger.info("[ëª¨ë¸ë¡œë“œì™„ë£Œ] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            model = create_sac_agent(env, args, logger)
        
        # 3. ì½œë°± ì„¤ì •
        callbacks, stats_collector = setup_callbacks(env, args, logger)
        
        # 4. í›ˆë ¨ ì‹¤í–‰
        train_agent(model, callbacks, args, logger)
        
        # 5. ëª¨ë¸ ì €ì¥
        save_model(model, args, logger)
        
        # 6. ìµœì¢… í‰ê°€ (í…ŒìŠ¤íŠ¸ìš© í™˜ê²½ ìƒì„±)
        test_env = create_environment(args, logger, is_test=True)
        episode_rewards, episode_lengths = evaluate_agent(
            model, test_env, logger, n_episodes=TEST_CONFIG['n_eval_episodes']
        )
        test_env.close()  # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬
        
        # 7. í›ˆë ¨ í†µê³„ ìˆ˜ì§‘ ë° ê²°ê³¼ ì €ì¥
        training_stats = stats_collector.get_training_stats()
        save_training_results(training_stats, episode_rewards, args, logger)
        
        logger.info("[ì™„ë£Œ] ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"[ì˜¤ë¥˜] í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    
    finally:
        # í™˜ê²½ ì •ë¦¬
        if 'env' in locals():
            env.close()
        logger.info("[ì •ë¦¬ì™„ë£Œ] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()
